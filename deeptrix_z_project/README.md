# DeepTrix-Z: A Deep Reinforcement Learning Tetris Bot

DeepTrix-Z is a project to develop a competitive Tetris bot using deep reinforcement learning. The bot is designed to master modern Tetris mechanics, aiming for high-level play characterized by:

*   All-spin setups (T-spins, TSTs, etc.)
*   Effective garbage canceling and countering.
*   High combo and Back-to-Back (B2B) chains.
*   Fast Attacks Per Minute (APM) and survival under pressure.
*   Real-time decision-making with hold and lookahead (currently 5 pieces).

This project is built with Python, using [OpenAI Gymnasium](https://gymnasium.farama.org/) for the Tetris environment and [Stable Baselines3](https://stable-baselines3.readthedocs.io/) for the Proximal Policy Optimization (PPO) reinforcement learning algorithm.

## Project Structure

*   `deeptrix_z/`: Core library for the Tetris environment and game logic.
    *   `game.py`: `TetrisEnv`, the main Gymnasium-compatible Tetris environment.
    *   `board.py`: Manages board state, piece mechanics, line clears, T-spin detection.
    *   `pieces.py`: Defines Tetrominoes, SRS rotation data, and piece generation (7-bag).
    *   `exceptions.py`: Custom game exceptions.
*   `scripts/`: Executable scripts for training and evaluation.
    *   `train_ppo.py`: Script to train the PPO agent.
    *   `evaluate_ppo.py`: Script to evaluate a trained PPO agent.
*   `AGENTS.md`: Guidelines and information for AI agents working on this codebase.
*   `requirements.txt`: Python package dependencies.
*   `ppo_tetris_logs/`: Default directory for TensorBoard logs and model checkpoints during training.
*   `ppo_tetris_model.zip`: Default filename for the saved trained model (generated by `train_ppo.py`).

## Getting Started

### Prerequisites

*   Python (3.9+ recommended)
*   Pip (Python package installer)

### Installation

1.  **Clone the repository (if applicable) or ensure you have the project files.**

2.  **Set up a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3.  **Install dependencies:**
    Navigate to the `deeptrix_z_project` directory (where `requirements.txt` is located) and run:
    ```bash
    pip install -r requirements.txt
    ```
    *Note: The requirements include PyTorch. If you have a specific CUDA version, you might want to install PyTorch manually first by following instructions on the [PyTorch website](https://pytorch.org/) to ensure GPU compatibility, then install the rest.*

### Training the Agent

To train the PPO agent, run the training script:
```bash
python scripts/train_ppo.py
```
*   Training progress will be logged to TensorBoard in the `ppo_tetris_logs` directory.
*   Models will be saved periodically in `ppo_tetris_logs` and the final model as `ppo_tetris_model.zip` (by default).
*   Training can take a significant amount of time and computational resources. Adjust `TOTAL_TIMESTEPS` in `train_ppo.py` as needed.

To view TensorBoard logs:
```bash
tensorboard --logdir=ppo_tetris_logs
```
Then open your browser to the URL provided by TensorBoard (usually `http://localhost:6006`).

### Evaluating a Trained Agent

To evaluate a trained model (e.g., `ppo_tetris_model.zip`), run the evaluation script:
```bash
python scripts/evaluate_ppo.py
```
*   Make sure the `MODEL_PATH` variable in `evaluate_ppo.py` points to your saved model file if it's different from the default.
*   The script will print out performance metrics after running a set number of evaluation episodes.

## Current Features

*   Gymnasium-compatible Tetris environment implementing:
    *   Standard 10x20 board (with hidden rows for piece spawning).
    *   7-bag piece generation.
    *   SRS-based piece rotations (kick data included).
    *   Hold piece mechanic.
    *   Line clears (single, double, triple, Tetris).
    *   Basic T-Spin detection (3-corner rule) and rewards.
    *   Combo and Back-to-Back (B2B) tracking and rewards.
    *   Perfect Clear detection and rewards.
    *   Configurable lookahead (next queue).
*   PPO agent training using Stable Baselines3.
    *   Utilizes `"MultiInputPolicy"` for dictionary-based observations (board state, ghost piece, current/hold/next pieces, game state scalars).
*   Reward function designed to incentivize modern Tetris strategies.
*   Basic training and evaluation scripts.
*   TensorBoard logging for training.

## Development Notes for Agents

Please refer to `AGENTS.md` for detailed guidelines on contributing to this project, coding conventions, and explanations of the system architecture.

## Future Enhancements
*   More sophisticated T-Spin detection (Mini vs. Full, specific kick tests).
*   Full garbage mechanics implementation (sending, receiving, canceling).
*   Rule-based teacher model for pre-training or reward shaping.
*   Hyperparameter optimization.
*   More comprehensive unit and integration testing.
*   Support for different Tetris rule sets or game modes.

Contributions and improvements are welcome!
```
